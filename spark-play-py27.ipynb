{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means in PySpark\n",
    "\n",
    "The next machine learning method I'd like to introduce is about clustering, K-means. It is an unsupervised learning method where we would like to group the observations into *K* groups (or subsets). We call it \"unsupervised\" since we don't have associated response measurements together with the observations to help check and evaluate the model we built (of course we can use other measures to evaluate the clustering models).\n",
    "\n",
    "K-means may be the simplest approach for clustering while it’s also an elegant and efficient method. To produce the clusters, K-means method only requires the number of clusters *K* as its input.\n",
    "\n",
    "The idea of K-means clustering is that a good clustering is with the smallest within-cluster variation (a measurement of how different the observations within a cluster are from each other) in a possible range. To achieve this purpose, K-means algorithm is designed in a \"greedy\" algorithm fashion\n",
    "\n",
    "**K-means Algorithm**\n",
    "\n",
    "\t1. For each observation, assign a random number which is generated from 1 to *K* to it.\n",
    "\n",
    "\t2. For each of the *K* clusters, compute the cluster center. The *k*th cluster’s center is the vector of the means of the vectors of all the observations belonging to the kth cluster.\n",
    "\n",
    "\t3. Re-assign each observation to the cluster whose cluster center is closest to this observation.\n",
    "\n",
    "\t4. Check if the new assignments are the same as the last iteration. If not, go to step 2; if yes, END.\n",
    "\n",
    "\n",
    "An example of iteration with K-means algorithm is presented below\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/XD-DENG/Spark-ML-Intro/master/figures/k-means.gif)\n",
    "\n",
    "\n",
    "Now it's time to implement K-means with PySpark. I generate a dateset myself, it contains 30 observations, and I purposedly \"made\" them group 3 sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "troubleshooting:\n",
    "- More: https://www.dataquest.io/blog/pyspark-installation-guide/\n",
    "- https://stackoverflow.com/questions/23280629/multiple-sparkcontexts-error-in-tutorial\n",
    "- check this tutorial he based it off of: http://blog.insightdatalabs.com/jupyter-on-apache-spark-step-by-step/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "- download for py27:\n",
    "```bash\n",
    "conda install numpy\n",
    "conda install pandas\n",
    "```\n",
    "\n",
    "- install numpy on the nodes:\n",
    "- in jupyter: https://blog.cloudera.com/blog/2015/09/how-to-prepare-your-apache-hadoop-cluster-for-pyspark-jobs/\n",
    "- https://docs.continuum.io/anaconda-cluster/manage-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "# # http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics\n",
    "# from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics, RankingMetrics\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "\n",
    "from random import randrange\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 519184\r\n",
      "drwxrwxr-x 20 ec2-user ec2-user      4096 Jun 28 17:43 anaconda3\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user 523283080 May 30 19:24 Anaconda3-4.4.0-Linux-x86_64.sh\r\n",
      "-rw-------  1 ec2-user ec2-user       678 Jun 28 17:53 aws_sf17ds6.pem\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user       675 Jun 29 03:33 derby.log\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user       503 Jun 29 03:32 jupyter-setup.sh\r\n",
      "drwxrwxr-x  5 ec2-user ec2-user      4096 Jun 29 03:33 metastore_db\r\n",
      "drwxrwxr-x  4 ec2-user ec2-user      4096 Jun 29 01:41 PICModel\r\n",
      "drwxrwxr-x 13 ec2-user ec2-user      4096 Jun 28 17:39 spark\r\n",
      "-rw-r--r--  1 ec2-user ec2-user     42774 Jun 28 18:50 spark-play.ipynb\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user    111400 Jun 29 03:30 spark-play-py27.ipynb\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user    473143 Jun 29 01:36 tmp-txns-no-headers.txt\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user    473183 Jun 29 01:17 tmp_txns.txt.1\r\n",
      "-rw-r--r--  1 ec2-user ec2-user   7211502 Jun 28 18:00 transactions.csv\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user       607 Jun 28 18:58 trial-py27.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f863a87a890>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc = SparkContext(\"local\", \"Simple App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc.stop(sc)\n",
    "# sc.getOrCreate(\"local\", \"Simple App\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Numpy on the cluster\n",
    "- required for Kmeans\n",
    "\n",
    "from local machine:\n",
    "```bash\n",
    "# install dependencies on the cluster ==================================\n",
    "flintrock run-command test-cluster 'sudo yum install -y gcc'\n",
    "# have faith...flintrock takes awhile (3-5 min)\n",
    "flintrock run-command test-cluster 'pip install --user numpy' \n",
    "```\n",
    "\n",
    "note to self: learn how to run bash scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Real Transaction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphX in PySpark -----------------------------------------------------\n",
    "- https://stackoverflow.com/questions/23302270/how-do-i-run-graphx-with-python-pyspark\n",
    "- GraphFrames is the way to go\n",
    "  - Work flow: http://aosc.umd.edu/~ide/data/teaching/amsc663/14fall/amsc663_14proposalpresentation_stefan_poikonen.pdf\n",
    "    - Pandas DataFrames & features --> graphframes\n",
    "    - graph analytics --> export as features\n",
    "\n",
    "## Real Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a502134dd601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transactions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ms'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#coerce date format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "transactions = pd.read_csv('transactions.csv')\n",
    "transactions['Date'] = pd.to_datetime(transactions['Date'],unit='ms') #coerce date format\n",
    "transactions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions columns:  ['Amount $', 'Date', 'Destination', 'Source', 'Transaction ID', 'isTainted']\n"
     ]
    }
   ],
   "source": [
    "print('transactions columns: ', list(transactions.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame --> GraphFrame\n",
    "from: http://www.datareply.co.uk/blog/2016/9/20/running-graph-analytics-with-spark-graphframes-a-simple-example\n",
    "\n",
    "## Loading the Data - Nodes\n",
    "We will use the pandas package for loading the dataset as a dataframe.\n",
    "- Though not explicitly necessary in this case, the code I am using below accounts for cases where the file size is too big to be loaded in one go and performs the loading operation in chunks.\n",
    "- We first load the genes with the following code.\n",
    "- Notice that since the .csv file contains relationships, genes under the `OFFICIAL_SYMBOL_A` header may appear multiple times and thus duplicates need to be removed for unique identifiers\n",
    "- **Make sure to name the dataframe's single column that we are importing with the header \"id\" otherwise you will get an exception when you try to instantiate the GraphFrame based on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#establish sql context\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Instantiate SQL_SparkContext object\n",
    "SQL_CONTEXT = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes = pd.read_csv('transactions.csv',\n",
    "                    usecols=['Source', 'isTainted'],\n",
    "                    low_memory=True,\n",
    "                    iterator=True,\n",
    "                    chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate chunks into list & convert to DataFrame\n",
    "nodes = pd.DataFrame(pd.concat(list(nodes), ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes columns:  ['id', 'isTainted']\n"
     ]
    }
   ],
   "source": [
    "# Create a Vertex DataFrame with unique ID column \"id\"\n",
    "nodes.columns = ['id', 'isTainted']\n",
    "print('nodes columns: ', list(nodes.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'2dd13954e18508bb8b3a41d96a022be9...', isTainted=0),\n",
       " Row(id=u'7c74d3afb41e536e26948a1d2455a7c7...', isTainted=0),\n",
       " Row(id=u'50dced19b8ee41114916bf3ca894f455...', isTainted=0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NODES = SQL_CONTEXT.createDataFrame(nodes)\n",
    "NODES.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parallelize -----------------------------------------------------\n",
    "# VERTICES = sc.parallelize(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = array([observation_group_1, observation_group_2, observation_group_3]).reshape(n_in_each_group*3, 5)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Generate the observations -----------------------------------------------------\n",
    "# n_in_each_group = 10   # how many observations in each group\n",
    "# n_of_feature = 5 # how many features we have for each observation\n",
    "\n",
    "# observation_group_1=[]\n",
    "# for i in range(n_in_each_group*n_of_feature):\n",
    "# \tobservation_group_1.append(randrange(5, 8))\n",
    "\n",
    "# observation_group_2=[]\n",
    "# for i in range(n_in_each_group*n_of_feature):\n",
    "# \tobservation_group_2.append(randrange(55, 58))\n",
    "\n",
    "# observation_group_3=[]\n",
    "# for i in range(n_in_each_group*n_of_feature):\n",
    "# \tobservation_group_3.append(randrange(105, 108))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del GENES_DF_CLEAN, GENES_DF, GENES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data - Edges\n",
    "\n",
    "\n",
    "We repeat the same process as before with the difference that now we will be importing all three columns. If I am to be strict, I should say that this step is not really necessary if one opts for loading everything in one go the first time, then SELECTing different column-dataframes thus efficiently differentiating between vertices and edges. Still, I am repeating the code here for presentation convenience. Also, even though it is not really necessary in our case, you should make sure that once you are done with the loading process you get rid of anything that should not be kept in the memory any more, e.g. run:  \n",
    "```spark\n",
    "del GENES_DF_CLEAN, GENES_DF, GENES\n",
    "```\n",
    "\n",
    "Side note: The del statement does not directly reclaim memory. It only, removes a reference, which decrements the reference count on the value and if that count is zero, the memory can be reclaimed. In general, CPython will reclaim the memory immediately, there's no need to wait for the garbage collector to run.\n",
    "\n",
    "Here is the code for loading the edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = pd.read_csv('transactions.csv',\n",
    "                    usecols=['Amount $', 'Date', 'Destination', 'Source', 'Transaction ID', 'isTainted'],\n",
    "                    low_memory=True,\n",
    "                    iterator=True,\n",
    "                    chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate chunks into list & convert to DataFrame\n",
    "edges = pd.DataFrame(pd.concat(list(edges), ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['Source', 'Destination', 'isTainted', 'Amount $', 'Date', 'Transaction ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges columns:  ['Source', 'Destination', 'isTainted', 'Amount $', 'Date', 'Transaction ID']\n"
     ]
    }
   ],
   "source": [
    "edges = edges[cols]\n",
    "print('edges columns: ', list(edges.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges columns:  ['src', 'dst', 'relationship', 'Amount $', 'Date', 'TxID']\n"
     ]
    }
   ],
   "source": [
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "edges.columns = [\"src\", \"dst\", \"relationship\", 'Amount $', 'Date', 'TxID']\n",
    "print('edges columns: ', list(edges.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(src=u'2dd13954e18508bb8b3a41d96a022be9...', dst=u'84a0b53e1ac008b8dd0fd6212d4b7fa2...', relationship=0, Amount $=3223.9752, Date=1385240000000.0, TxID=u'b6eb8ba20df31fa74fbe7755f58c18f82a599d6bb5fa79ef670da27e793a25fd'),\n",
       " Row(src=u'7c74d3afb41e536e26948a1d2455a7c7...', dst=u'3b62a891b99969042d4e6ac8158d0a18...', relationship=0, Amount $=3708.0216, Date=1401500000000.0, TxID=u'60df3c67063e136a0c9715edcd12ae717e6f9ed492afe2140294a1f283ddfa03'),\n",
       " Row(src=u'50dced19b8ee41114916bf3ca894f455...', dst=u'3b62a891b99969042d4e6ac8158d0a18...', relationship=0, Amount $=2.48, Date=1398560000000.0, TxID=u'a6aafd3d85600844536b8a5f2c255686c33dc4969e68a45ca0978e2f00977322')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDGES = SQL_CONTEXT.createDataFrame(edges)\n",
    "EDGES.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.sql.dataframe.DataFrame, pyspark.sql.dataframe.DataFrame)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(NODES), type(EDGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next we finally create the graph:\n",
    "g = GraphFrame(NODES, EDGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analytics\n",
    "- More examples: http://graphframes.github.io/user-guide.html\n",
    "- MasterNodePublicDNS: http://ec2-54-213-254-250.us-west-2.compute.amazonaws.com:8080/\n",
    "\n",
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45117\n",
      "45117\n",
      "28832\n",
      "543\n"
     ]
    }
   ],
   "source": [
    "# Query: Count the number of \"isTainted\" connections in the graph.\n",
    "print(g.vertices.count())\n",
    "print(g.edges.count())\n",
    "print(g.degrees.count())\n",
    "print(g.vertices.filter(\"isTainted = 5\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put a feature in pandas - put it back into pandas if resorting to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex in-Degree -----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount $</th>\n",
       "      <th>Date</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Source</th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>isTainted</th>\n",
       "      <th>id</th>\n",
       "      <th>inDegree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3223.9752</td>\n",
       "      <td>2013-11-23 20:53:20</td>\n",
       "      <td>84a0b53e1ac008b8dd0fd6212d4b7fa2...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>b6eb8ba20df31fa74fbe7755f58c18f82a599d6bb5fa79...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984.0000</td>\n",
       "      <td>2013-11-21 07:46:40</td>\n",
       "      <td>c283daf07c2c146fa8947d7306279cb0...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>8ce957cb97826ddff443380934cb7dd297ac2684177da4...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4094.7032</td>\n",
       "      <td>2013-11-09 18:00:00</td>\n",
       "      <td>9550de4ed8a7b13a5b5252061cdf9b63...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>df07a3749ee5bae84817079f7a4b4cf2fcdfe0924c6526...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3719.9752</td>\n",
       "      <td>2013-12-09 03:33:20</td>\n",
       "      <td>d0770b7769567b0d0c822203f5858689...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>de8095f19d32a58f2395c9a62b75ec0427fc83a635f953...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5208.0000</td>\n",
       "      <td>2014-01-03 11:53:20</td>\n",
       "      <td>09f3142d9c75be2d1fe94e9471186547...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>a240e4c369fe4c8c6874cd9c5e5fc4113a8bc206f8bda6...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Amount $                Date                          Destination  \\\n",
       "0  3223.9752 2013-11-23 20:53:20  84a0b53e1ac008b8dd0fd6212d4b7fa2...   \n",
       "1  1984.0000 2013-11-21 07:46:40  c283daf07c2c146fa8947d7306279cb0...   \n",
       "2  4094.7032 2013-11-09 18:00:00  9550de4ed8a7b13a5b5252061cdf9b63...   \n",
       "3  3719.9752 2013-12-09 03:33:20  d0770b7769567b0d0c822203f5858689...   \n",
       "4  5208.0000 2014-01-03 11:53:20  09f3142d9c75be2d1fe94e9471186547...   \n",
       "\n",
       "                                Source  \\\n",
       "0  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "1  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "2  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "3  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "4  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "\n",
       "                                      Transaction ID  isTainted  \\\n",
       "0  b6eb8ba20df31fa74fbe7755f58c18f82a599d6bb5fa79...          0   \n",
       "1  8ce957cb97826ddff443380934cb7dd297ac2684177da4...          0   \n",
       "2  df07a3749ee5bae84817079f7a4b4cf2fcdfe0924c6526...          0   \n",
       "3  de8095f19d32a58f2395c9a62b75ec0427fc83a635f953...          0   \n",
       "4  a240e4c369fe4c8c6874cd9c5e5fc4113a8bc206f8bda6...          0   \n",
       "\n",
       "                                    id  inDegree  \n",
       "0  2dd13954e18508bb8b3a41d96a022be9...         1  \n",
       "1  2dd13954e18508bb8b3a41d96a022be9...         1  \n",
       "2  2dd13954e18508bb8b3a41d96a022be9...         1  \n",
       "3  2dd13954e18508bb8b3a41d96a022be9...         1  \n",
       "4  2dd13954e18508bb8b3a41d96a022be9...         1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query: Get in-degree of each vertex.\n",
    "print(\"Vertex in-Degree -----------------------------------------------------------------------\")\n",
    "df = g.inDegrees.sort('inDegree', ascending=False).toPandas()\n",
    "transactions = transactions.merge(df,\n",
    "                                  left_on='Source',\n",
    "                                  right_on='id',)\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex out-Degree ----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount $</th>\n",
       "      <th>Date</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Source</th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>isTainted</th>\n",
       "      <th>id_x</th>\n",
       "      <th>inDegree</th>\n",
       "      <th>id_y</th>\n",
       "      <th>outDegree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3223.9752</td>\n",
       "      <td>2013-11-23 20:53:20</td>\n",
       "      <td>84a0b53e1ac008b8dd0fd6212d4b7fa2...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>b6eb8ba20df31fa74fbe7755f58c18f82a599d6bb5fa79...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984.0000</td>\n",
       "      <td>2013-11-21 07:46:40</td>\n",
       "      <td>c283daf07c2c146fa8947d7306279cb0...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>8ce957cb97826ddff443380934cb7dd297ac2684177da4...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4094.7032</td>\n",
       "      <td>2013-11-09 18:00:00</td>\n",
       "      <td>9550de4ed8a7b13a5b5252061cdf9b63...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>df07a3749ee5bae84817079f7a4b4cf2fcdfe0924c6526...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3719.9752</td>\n",
       "      <td>2013-12-09 03:33:20</td>\n",
       "      <td>d0770b7769567b0d0c822203f5858689...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>de8095f19d32a58f2395c9a62b75ec0427fc83a635f953...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5208.0000</td>\n",
       "      <td>2014-01-03 11:53:20</td>\n",
       "      <td>09f3142d9c75be2d1fe94e9471186547...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>a240e4c369fe4c8c6874cd9c5e5fc4113a8bc206f8bda6...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Amount $                Date                          Destination  \\\n",
       "0  3223.9752 2013-11-23 20:53:20  84a0b53e1ac008b8dd0fd6212d4b7fa2...   \n",
       "1  1984.0000 2013-11-21 07:46:40  c283daf07c2c146fa8947d7306279cb0...   \n",
       "2  4094.7032 2013-11-09 18:00:00  9550de4ed8a7b13a5b5252061cdf9b63...   \n",
       "3  3719.9752 2013-12-09 03:33:20  d0770b7769567b0d0c822203f5858689...   \n",
       "4  5208.0000 2014-01-03 11:53:20  09f3142d9c75be2d1fe94e9471186547...   \n",
       "\n",
       "                                Source  \\\n",
       "0  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "1  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "2  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "3  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "4  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "\n",
       "                                      Transaction ID  isTainted  \\\n",
       "0  b6eb8ba20df31fa74fbe7755f58c18f82a599d6bb5fa79...          0   \n",
       "1  8ce957cb97826ddff443380934cb7dd297ac2684177da4...          0   \n",
       "2  df07a3749ee5bae84817079f7a4b4cf2fcdfe0924c6526...          0   \n",
       "3  de8095f19d32a58f2395c9a62b75ec0427fc83a635f953...          0   \n",
       "4  a240e4c369fe4c8c6874cd9c5e5fc4113a8bc206f8bda6...          0   \n",
       "\n",
       "                                  id_x  inDegree  \\\n",
       "0  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "1  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "2  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "3  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "4  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "\n",
       "                                  id_y  outDegree  \n",
       "0  2dd13954e18508bb8b3a41d96a022be9...         16  \n",
       "1  2dd13954e18508bb8b3a41d96a022be9...         16  \n",
       "2  2dd13954e18508bb8b3a41d96a022be9...         16  \n",
       "3  2dd13954e18508bb8b3a41d96a022be9...         16  \n",
       "4  2dd13954e18508bb8b3a41d96a022be9...         16  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vertex out-Degree ----------------------------------------------------------------------\")\n",
    "df = g.outDegrees.sort('outDegree', ascending=False).toPandas()\n",
    "transactions = transactions.merge(df,\n",
    "                                  left_on='Source',\n",
    "                                  right_on='id')\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex degree --------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount $</th>\n",
       "      <th>Date</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Source</th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>isTainted</th>\n",
       "      <th>id_x</th>\n",
       "      <th>inDegree</th>\n",
       "      <th>id_y</th>\n",
       "      <th>outDegree</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3223.9752</td>\n",
       "      <td>2013-11-23 20:53:20</td>\n",
       "      <td>84a0b53e1ac008b8dd0fd6212d4b7fa2...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>b6eb8ba20df31fa74fbe7755f58c18f82a599d6bb5fa79...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984.0000</td>\n",
       "      <td>2013-11-21 07:46:40</td>\n",
       "      <td>c283daf07c2c146fa8947d7306279cb0...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>8ce957cb97826ddff443380934cb7dd297ac2684177da4...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4094.7032</td>\n",
       "      <td>2013-11-09 18:00:00</td>\n",
       "      <td>9550de4ed8a7b13a5b5252061cdf9b63...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>df07a3749ee5bae84817079f7a4b4cf2fcdfe0924c6526...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3719.9752</td>\n",
       "      <td>2013-12-09 03:33:20</td>\n",
       "      <td>d0770b7769567b0d0c822203f5858689...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>de8095f19d32a58f2395c9a62b75ec0427fc83a635f953...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5208.0000</td>\n",
       "      <td>2014-01-03 11:53:20</td>\n",
       "      <td>09f3142d9c75be2d1fe94e9471186547...</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>a240e4c369fe4c8c6874cd9c5e5fc4113a8bc206f8bda6...</td>\n",
       "      <td>0</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>1</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>16</td>\n",
       "      <td>2dd13954e18508bb8b3a41d96a022be9...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Amount $                Date                          Destination  \\\n",
       "0  3223.9752 2013-11-23 20:53:20  84a0b53e1ac008b8dd0fd6212d4b7fa2...   \n",
       "1  1984.0000 2013-11-21 07:46:40  c283daf07c2c146fa8947d7306279cb0...   \n",
       "2  4094.7032 2013-11-09 18:00:00  9550de4ed8a7b13a5b5252061cdf9b63...   \n",
       "3  3719.9752 2013-12-09 03:33:20  d0770b7769567b0d0c822203f5858689...   \n",
       "4  5208.0000 2014-01-03 11:53:20  09f3142d9c75be2d1fe94e9471186547...   \n",
       "\n",
       "                                Source  \\\n",
       "0  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "1  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "2  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "3  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "4  2dd13954e18508bb8b3a41d96a022be9...   \n",
       "\n",
       "                                      Transaction ID  isTainted  \\\n",
       "0  b6eb8ba20df31fa74fbe7755f58c18f82a599d6bb5fa79...          0   \n",
       "1  8ce957cb97826ddff443380934cb7dd297ac2684177da4...          0   \n",
       "2  df07a3749ee5bae84817079f7a4b4cf2fcdfe0924c6526...          0   \n",
       "3  de8095f19d32a58f2395c9a62b75ec0427fc83a635f953...          0   \n",
       "4  a240e4c369fe4c8c6874cd9c5e5fc4113a8bc206f8bda6...          0   \n",
       "\n",
       "                                  id_x  inDegree  \\\n",
       "0  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "1  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "2  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "3  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "4  2dd13954e18508bb8b3a41d96a022be9...         1   \n",
       "\n",
       "                                  id_y  outDegree  \\\n",
       "0  2dd13954e18508bb8b3a41d96a022be9...         16   \n",
       "1  2dd13954e18508bb8b3a41d96a022be9...         16   \n",
       "2  2dd13954e18508bb8b3a41d96a022be9...         16   \n",
       "3  2dd13954e18508bb8b3a41d96a022be9...         16   \n",
       "4  2dd13954e18508bb8b3a41d96a022be9...         16   \n",
       "\n",
       "                                    id  degree  \n",
       "0  2dd13954e18508bb8b3a41d96a022be9...      17  \n",
       "1  2dd13954e18508bb8b3a41d96a022be9...      17  \n",
       "2  2dd13954e18508bb8b3a41d96a022be9...      17  \n",
       "3  2dd13954e18508bb8b3a41d96a022be9...      17  \n",
       "4  2dd13954e18508bb8b3a41d96a022be9...      17  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vertex degree --------------------------------------------------------------------------\")\n",
    "df = g.degrees.sort('degree', ascending=False).toPandas()\n",
    "transactions = transactions.merge(df,\n",
    "                                  left_on='Source',\n",
    "                                  right_on='id')\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions = transactions.drop(['id_x', 'id_y', 'id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Queries - THIS IS WHERE THINGS BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triangle Count -------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# hits no space left on device\n",
    "print(\"Triangle Count -------------------------------------------------------------------------\")\n",
    "RESULTS = g.triangleCount()\n",
    "df = RESULTS.select(\"id\", \"count\").toPandas()\n",
    "transactions = transactions.merge(df,\n",
    "                                  left_on='Source',\n",
    "                                  right_on='id')\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Label Propagation ----------------------------------------------------------------------\")\n",
    "# Convergence is not guaranteed\n",
    "df = g.labelPropagation(maxIter=10).toPandas()\n",
    "# transactions = transactions.merge(df,\n",
    "#                                   left_on='Source',\n",
    "#                                   right_on='id')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two implementations of PageRank.\n",
    "- The first implementation uses the standalone GraphFrame interface and runs PageRank for a fixed number of iterations. This can be run by setting maxIter.\n",
    "- The second implementation uses the org.apache.spark.graphx.Pregel interface and runs PageRank until convergence. This can be run by setting tol.\n",
    "**Both implementations support non-personalized and personalized PageRank, where setting a sourceId personalizes the results for that vertex.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|           pagerank|\n",
      "+---+-------------------+\n",
      "|  b| 0.2808611427228327|\n",
      "|  a|               0.01|\n",
      "|  c|0.27995525261339177|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Run PageRank algorithm the other one, and show results.\n",
    "# results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
    "# results.vertices.select(\"id\", \"pagerank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run PageRank algorithm (takes awhile), and show results.\n",
    "print(\"PageRank -------------------------------------------------------------------------------\")\n",
    "df = g.pageRank(resetProbability=0.15, tol=0.01)\\\n",
    "  .vertices.sort('pagerank', ascending=False).toPandas()\n",
    "# transactions = transactions.merge(df,\n",
    "#                                   left_on='Source',\n",
    "#                                   right_on='id')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Find Shortest Paths w.r.t. Tainted Wallets ---------------------------------------------------\")\n",
    "SHORTEST_PATH = g.shortestPaths(landmarks=[\"5\"])\n",
    "df = SHORTEST_PATH.select(\"id\", \"distances\").toPandas()\n",
    "# transactions = transactions.merge(df,\n",
    "#                                   left_on='Source',\n",
    "#                                   right_on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porting the graphframe (if necessary)\n",
    "- http://graphframes.github.io/user-guide.html#saving-and-loading-graphframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster like your life depends on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions columns:  ['Amount $', 'Date', 'Destination', 'Source', 'Transaction ID', 'isTainted', 'inDegree', 'outDegree', 'degree']\n"
     ]
    }
   ],
   "source": [
    "print('transactions columns: ', list(transactions.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['inDegree', 'outDegree', 'degree']\n",
    "tmp_transactions = transactions[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_int(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:  \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for column in tmp_transactions.columns:\n",
    "    tmp_transactions[column] = tmp_transactions[column].apply(string_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26050 entries, 0 to 26049\n",
      "Data columns (total 3 columns):\n",
      "inDegree     26050 non-null int64\n",
      "outDegree    26050 non-null int64\n",
      "degree       26050 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 814.1 KB\n"
     ]
    }
   ],
   "source": [
    "tmp_transactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(tmp_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for MultinomialNB classification\n",
    "tmp_transactions.to_csv('tmp-txns-no-headers.txt', header=False, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anaconda3\t\t\t metastore_db\t\ttmp-txns-no-headers.txt\r\n",
      "Anaconda3-4.4.0-Linux-x86_64.sh  PICModel\t\ttmp_txns.txt\r\n",
      "aws_sf17ds6.pem\t\t\t spark\t\t\ttmp_txns.txt.1\r\n",
      "derby.log\t\t\t spark-play.ipynb\ttransactions.csv\r\n",
      "jupyter-setup.sh\t\t spark-play-py27.ipynb\ttrial-py27.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Run the K-Means algorithm -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o206.trainKMeansModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 31 in stage 28.0 failed 4 times, most recent failure: Lost task 31.3 in stage 28.0 (TID 1770, 172.31.37.199, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <type 'str'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeSample$1.apply(RDD.scala:568)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.takeSample(RDD.scala:557)\n\tat org.apache.spark.mllib.clustering.KMeans.initRandom(KMeans.scala:334)\n\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:254)\n\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:227)\n\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:209)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainKMeansModel(PythonMLLibAPI.scala:367)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <type 'str'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f4ba7ef8193f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the K-Means model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# the initializationMode can also be \"k-means||\" or set by users.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializationMode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/spark/python/pyspark/mllib/clustering.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, rdd, k, maxIterations, runs, initializationMode, seed, initializationSteps, epsilon, initialModel)\u001b[0m\n\u001b[1;32m    354\u001b[0m         model = callMLlibFunc(\"trainKMeansModel\", rdd.map(_convert_to_vector), k, maxIterations,\n\u001b[1;32m    355\u001b[0m                               \u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializationMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializationSteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                               clusterInitialModel)\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclusterCenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mKMeansModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/spark/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/spark/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o206.trainKMeansModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 31 in stage 28.0 failed 4 times, most recent failure: Lost task 31.3 in stage 28.0 (TID 1770, 172.31.37.199, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <type 'str'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeSample$1.apply(RDD.scala:568)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.takeSample(RDD.scala:557)\n\tat org.apache.spark.mllib.clustering.KMeans.initRandom(KMeans.scala:334)\n\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:254)\n\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:227)\n\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:209)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainKMeansModel(PythonMLLibAPI.scala:367)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <type 'str'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Build the K-Means model\n",
    "# the initializationMode can also be \"k-means||\" or set by users.\n",
    "clusters = KMeans.train(data, 2, maxIterations=3, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect the clustering result\n",
    "result=data.map(lambda point: clusters.predict(point)).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = data.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PowerIteration Clustering -----------------------------------------------------\n",
    "**Power Iteration Clustering**\n",
    "- https://spark.apache.org/docs/2.1.0/mllib-clustering.html#power-iteration-clustering-pic\n",
    "- http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns are : id,inDegree,outDegree,degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import PowerIterationClustering, PowerIterationClusteringModel\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"tmp-txns-no-headers.txt\")\n",
    "similarities = data.map(lambda line: tuple([float(x) for x in line.split(',')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cluster the data into two classes using PowerIterationClustering\n",
    "model = PowerIterationClustering.train(similarities, k=2, maxIterations=10, initMode='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Assignment(id=384, cluster=0),\n",
       " Assignment(id=18624, cluster=0),\n",
       " Assignment(id=9200, cluster=0)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "assignments = model.assignments().collect()\n",
    "assignments[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in assignments:\n",
    "    results.append([x.id, x.cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[384, 0], [18624, 0], [9200, 0]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "# model.save(sc, \"PICModel\")\n",
    "# sameModel = PowerIterationClusteringModel.load(sc, \"PICModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, index=None, columns = ['id', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DOUBLE BRACKETS CREATES A DF\n",
    "merged = transactions[['isTainted']].merge(results_df, left_index=True, right_on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation on Local Machine\n",
    "- Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm.\n",
    "- In particular any evaluation metric should not take the absolute values of the cluster labels into account, **but rather, if this clustering defines separations of the data similar to some ground truth set of classes**\n",
    "- OR satisfying some assumption such that **(members belonging to the same class) are more similar that (members of different classes)** according to some similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"cluster-results.pkl\", 'rb') as picklefile: \n",
    "    results = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isTainted</th>\n",
       "      <th>id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10085</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11200</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10387</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       isTainted  id  cluster\n",
       "10085          0   0        1\n",
       "11200          0   1        0\n",
       "10387          0   2        0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_true(value):\n",
    "    if value == 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "results['isTainted'] = results['isTainted'].apply(convert_to_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isTainted</th>\n",
       "      <th>id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10079</th>\n",
       "      <td>1</td>\n",
       "      <td>5608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12253</th>\n",
       "      <td>1</td>\n",
       "      <td>5609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10190</th>\n",
       "      <td>1</td>\n",
       "      <td>5610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       isTainted    id  cluster\n",
       "10079          1  5608        1\n",
       "12253          1  5609        1\n",
       "10190          1  5610        1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['isTainted'] == 1].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, homogeneity_score, completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "TaintedWallet       1.00      1.00      1.00     22109\n",
      "       Wallet       0.99      1.00      1.00       237\n",
      "\n",
      "  avg / total       1.00      1.00      1.00     22346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = results['isTainted']\n",
    "y_pred = results['cluster']\n",
    "target_names = ['TaintedWallet', 'Wallet']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional entropy analyses on clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99119856200298073"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homogeneity: each cluster contains only members of a single class.\n",
    "homogeneity_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98440495116631499"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# completeness: all members of a given class are assigned to the same cluster.\n",
    "completeness_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- supervised learning techniques are rigid and don't take into account changing patterns.\n",
    "- Using clustering techniques in an unsupervised manner allows for adaptive fraud detection even if fraudsters change the behaviors that led to classification in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MultinomialNB Classification - tip from guo\n",
    "- possibly to determine feature importances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.NaiveBayesModel\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [LabeledPoint(0.0, [0.0, 0.0]),\n",
    "        LabeledPoint(0.0, [0.0, 1.0]),\n",
    "        LabeledPoint(1.0, [1.0, 0.0])]\n",
    "\n",
    "model = NaiveBayes.train(sc.parallelize(data))\n",
    "model.predict(array([0.0, 1.0]))\n",
    "model.predict(array([1.0, 0.0]))\n",
    "\n",
    "model.predict(sc.parallelize([[1.0, 0.0]])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_data = [LabeledPoint(0.0, SparseVector(2, {1: 0.0})),\n",
    "               LabeledPoint(0.0, SparseVector(2, {1: 1.0})),\n",
    "               LabeledPoint(1.0, SparseVector(2, {0: 1.0}))]\n",
    "\n",
    "model = NaiveBayes.train(sc.parallelize(sparse_data))\n",
    "model.predict(SparseVector(2, {1: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(SparseVector(2, {0: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOESNT WORK: Exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.mllib.classification.NaiveBayesModel.load.\n: java.lang.UnsupportedOperationException: empty collection\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1370)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1367)\n\tat org.apache.spark.mllib.util.Loader$.loadMetadata(modelSaveLoad.scala:129)\n\tat org.apache.spark.mllib.classification.NaiveBayesModel$.load(NaiveBayes.scala:271)\n\tat org.apache.spark.mllib.classification.NaiveBayesModel.load(NaiveBayes.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b7858ab4a343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msameModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayesModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msameModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/spark/python/pyspark/mllib/classification.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, sc, path)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[1;32m    639\u001b[0m         java_model = sc._jvm.org.apache.spark.mllib.classification.NaiveBayesModel.load(\n\u001b[0;32m--> 640\u001b[0;31m             sc._jsc.sc(), path)\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0;31m# Can not unpickle array.array from Pyrolite in Python3 with \"bytes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mpy_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.mllib.classification.NaiveBayesModel.load.\n: java.lang.UnsupportedOperationException: empty collection\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1370)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1367)\n\tat org.apache.spark.mllib.util.Loader$.loadMetadata(modelSaveLoad.scala:129)\n\tat org.apache.spark.mllib.classification.NaiveBayesModel$.load(NaiveBayes.scala:271)\n\tat org.apache.spark.mllib.classification.NaiveBayesModel.load(NaiveBayes.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "import os, tempfile\n",
    "path = tempfile.mkdtemp()\n",
    "\n",
    "model.save(sc, path)\n",
    "sameModel = NaiveBayesModel.load(sc, path)\n",
    "sameModel.predict(SparseVector(2, {0: 1.0})) == model.predict(SparseVector(2, {0: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "try:\n",
    "    rmtree(path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINISH LATER\n",
    "\n",
    "## Pairwise similarity matrix for PIC:\n",
    "- http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf\n",
    "- http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    "\n",
    "## if clustering on csvs, cut the frame down and use sklearn...\n",
    "  \n",
    "- Other options:\n",
    "  - https://github.com/SsureyMoon/Bitcoin-network-cluster \n",
    "  - https://github.com/JeremyRubin/BTCSpark\n",
    "  - https://github.com/mikispag/bitiodine\n",
    "  \n",
    "## for another time: Spark-YARN & performance tuning (set AMIs to m3.mediums)\n",
    "- https://docs.continuum.io/anaconda-scale/howto/spark-yarn\n",
    "- https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motif finding\n",
    "- http://graphframes.github.io/user-guide.html#motif-finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphframes.examples import Graphs\n",
    "g = Graphs(sqlContext).friends()  # Get example graph\n",
    "\n",
    "# Search for pairs of vertices with edges in both directions between them.\n",
    "motifs = g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\")\n",
    "motifs.show()\n",
    "\n",
    "# More complex queries can be expressed by applying filters.\n",
    "motifs.filter(\"b.age > 30\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "485px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "3",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
